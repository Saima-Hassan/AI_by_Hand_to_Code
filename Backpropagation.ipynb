{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Input vector\n",
    "X = np.array([[2], [1], [3]])\n",
    "\n",
    "# Layer 1 parameters (weights and biases)\n",
    "W1 = np.array([[1, -1, 1], [1, 1, 0], [0, 1, 1], [1, 0, 1]])\n",
    "b1 = np.array([-5, 0, 1, -2])\n",
    "b1= b1.reshape(-1,1) #reshape your array with 1 column and as many rows as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function: Rectified Linear Unit (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " z1:\n",
      " [[-1]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]]\n",
      " a1:\n",
      " [[0]\n",
      " [3]\n",
      " [5]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through Layer 1\n",
    "z1 = np.dot(W1, X) + b1\n",
    "a1 = relu(z1)\n",
    "print(\" z1:\\n\", z1)\n",
    "print(\" a1:\\n\", a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " z2:\n",
      " [[2]\n",
      " [4]]\n",
      " a2:\n",
      " [[2]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# Layer 2 parameters\n",
    "W2 = np.array([[1, -1, 1, 0],\n",
    "               [0, 1, -1, 1]])\n",
    "b2 = np.array([0, 3])\n",
    "b2= b2.reshape(-1,1)\n",
    "\n",
    "# Forward pass through Layer 2\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "a2 = relu(z2)\n",
    "\n",
    "print(\" z2:\\n\", z2)\n",
    "print(\" a2:\\n\", a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3 parameters\n",
    "W3 = np.array([[2, 0],\n",
    "               [0, 2],\n",
    "               [1, 1]])\n",
    "b3 = np.array([-1, -5, -7])\n",
    "b3= b3.reshape(-1,1)\n",
    "\n",
    "# Forward pass through Layer 3\n",
    "z3 = np.dot(W3, a2) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(values):\n",
    " \n",
    "    # Computing element wise exponential value\n",
    "    exp_values = np.exp(values)\n",
    " \n",
    "    # Computing sum of these values\n",
    "    exp_values_sum = np.sum(exp_values)\n",
    " \n",
    "    # Returing the softmax output.\n",
    "    return exp_values/exp_values_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a3:\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "a3 =np.round(softmax(z3),1)\n",
    "print(\" a3:\\n\", a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and target values\n",
    "Y_pred = a3\n",
    "Y_target = np.array([0, 1, 0])\n",
    "Y_target= Y_target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the ReLU function\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(x.dtype) #converts the boolean array to the same data type as x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dL_dz3:\n",
      " [[ 0.5]\n",
      " [-0.5]\n",
      " [ 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Begin backpropagation\n",
    "\n",
    "# Compute the gradient of the loss with respect to z3 (Layer 3 pre-activation)\n",
    "dL_dz3 = Y_pred - Y_target\n",
    "print(\" ∂L_∂z3:\\n\", dL_dz3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ∂L/∂W3:\n",
      " [[ 1. -1.  0.]\n",
      " [ 2. -2.  0.]]\n",
      " ∂L/∂b3:\n",
      " [[ 0.5 -0.5  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradients of the loss with respect to Layer 3 weights and biases\n",
    "dL_dW3 = np.dot(dL_dz3.reshape(-1, 1), a2.T)\n",
    "#dL_dW3 =np.round(dL_dW3,1)\n",
    "dL_db3 = dL_dz3.T\n",
    "\n",
    "print(\" ∂L/∂W3:\\n\", dL_dW3.T)\n",
    "print(\" ∂L/∂b3:\\n\", dL_db3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the gradient of the loss with respect to a2 (Layer 2 activation)\n",
    "dL_da2 = np.dot(W3.T, dL_dz3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ∂L/∂b2:\n",
      " [[ 1.]\n",
      " [-1.]]\n",
      " ∂L/∂W2:\n",
      " [[ 0.  0.]\n",
      " [ 3. -3.]\n",
      " [ 5. -5.]\n",
      " [ 3. -3.]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient of the loss with respect to z2 (Layer 2 pre-activation)\n",
    "dL_dz2 = dL_da2\n",
    "\n",
    "# Compute the gradients of the loss with respect to Layer 2 weights and biases\n",
    "dL_dW2 = np.dot(dL_dz2.reshape(-1, 1), a1.T)\n",
    "dL_db2 = dL_dz2\n",
    "\n",
    "print(\" ∂L/∂b2:\\n\", dL_db2)\n",
    "print(\" ∂L/∂W2:\\n\", dL_dW2.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-2.],\n",
       "       [ 2.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the gradient of the loss with respect to a1 (Layer 1 activation)\n",
    "dL_da1 = np.dot(W2.T, dL_dz2)\n",
    "dL_da1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ∂L/∂z1:\n",
      " [[ 0. -2.  2. -1.]]\n",
      " ∂L/∂b1:\n",
      " [[ 0. -2.  2. -1.]]\n",
      " ∂L/∂W1:\n",
      " [[ 0. -4.  4. -2.]\n",
      " [ 0. -2.  2. -1.]\n",
      " [ 0. -6.  6. -3.]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradient of the loss with respect to a1 (Layer 1 activation)\n",
    "dL_da1 = np.dot(W2.T, dL_dz2)\n",
    "\n",
    "\n",
    "# Apply the derivative of ReLU to the backpropagated gradient\n",
    "# to compute the gradient of the loss with respect to z1 (Layer 1 pre-activation)\n",
    "dL_dz1 = dL_da1.T * relu_derivative(z1.ravel())\n",
    "print(\" ∂L/∂z1:\\n\", dL_dz1)\n",
    "# Compute the gradients of the loss with respect to Layer 1 biases\n",
    "# This is the gradient of the loss with respect to z1 itself since we have a single sample\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "# Compute the gradients of the loss with respect to Layer 1 weights\n",
    "# by outer product of the input X and dL_dz1\n",
    "dL_dW1 = np.dot(X, dL_dz1.reshape(1, -1))\n",
    "\n",
    "print(\" ∂L/∂b1:\\n\", dL_db1)\n",
    "print(\" ∂L/∂W1:\\n\", dL_dW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the gradients for each layer to verify the backpropagation process\n",
    "print(\"Layer 3 gradients:\")\n",
    "print(\" z3:\\n\", z3.T)\n",
    "print(\" ∂L/∂b3:\\n\", dL_db3)\n",
    "print(\" ∂L/∂W3:\\n\", dL_dW3.T)\n",
    "\n",
    "print(\"Layer 2 gradients:\")\n",
    "print(\" z2:\\n\", z2.T)\n",
    "print(\" a2:\\n\", a2.T)\n",
    "print(\" ∂L/∂b2:\\n\", dL_db2)\n",
    "print(\" ∂L/∂W2:\\n\", dL_dW2.T)\n",
    "\n",
    "print(\"Layer 1 gradients:\")\n",
    "print(\" z1:\\n\", z1.T)\n",
    "print(\" a1:\\n\", a1.T)\n",
    "print(\" ∂L/∂b1:\\n\", dL_db1)\n",
    "print(\" ∂L/∂W1:\\n\", dL_dW1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
